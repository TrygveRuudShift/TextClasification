{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 19:18:54.419259: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-05 19:18:54.498368: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-05 19:18:54.499455: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-05 19:18:56.180434: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
    "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), \n",
    "             Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 3s 0us/step\n",
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "\n",
    "(x_train, y_train), (x_val, y_val) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  CB_Label\n",
      "0  Subject: stock promo mover : cwtd\\n * * * urge...         1\n",
      "1  Subject: are you listed in major search engine...         1\n",
      "2  Subject: important information thu , 30 jun 20...         1\n",
      "3  Subject: = ? utf - 8 ? q ? bask your life with...         1\n",
      "4  Subject: \" bidstogo \" is places to go , things...         1\n"
     ]
    }
   ],
   "source": [
    "# read data.csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# df = pd.read_csv('data.csv')\n",
    "df = pd.read_csv('spam_email.csv')\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 Training sequences\n",
      "2000 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentences\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "\n",
    "# Obtain the corresponding word indices\n",
    "x_train = tokenizer.texts_to_sequences(df['text'])\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "\n",
    "# Obtain the corresponding labels\n",
    "y_train = df['label'].values\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)\n",
    "\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(20, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outputs = Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 21s 143ms/step - loss: 0.3325 - accuracy: 0.8420 - val_loss: 0.0974 - val_accuracy: 0.9570\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 17s 134ms/step - loss: 0.0715 - accuracy: 0.9743 - val_loss: 0.0823 - val_accuracy: 0.9685\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 18s 147ms/step - loss: 0.0348 - accuracy: 0.9880 - val_loss: 0.0500 - val_accuracy: 0.9835\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 20s 163ms/step - loss: 0.0146 - accuracy: 0.9945 - val_loss: 0.0501 - val_accuracy: 0.9830\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 18s 143ms/step - loss: 0.0081 - accuracy: 0.9969 - val_loss: 0.0442 - val_accuracy: 0.9895\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 0.0446 - val_accuracy: 0.9885\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0526 - val_accuracy: 0.9880\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 18s 147ms/step - loss: 0.0057 - accuracy: 0.9986 - val_loss: 0.0749 - val_accuracy: 0.9815\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 19s 154ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0687 - val_accuracy: 0.9850\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 18s 146ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.0722 - val_accuracy: 0.9845\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=64, epochs=10, \n",
    "                    validation_data=(x_val, y_val)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"predict_class.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 - 2s - loss: 0.0722 - accuracy: 0.9845 - 2s/epoch - 26ms/step\n",
      "loss: 0.072\n",
      "accuracy: 0.984\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_val, y_val, verbose=2)\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "    print(\"%s: %.3f\" % (name, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 334ms/step\n",
      "[[7.875095e-04 9.992125e-01]]\n",
      "The sentence is spam\n"
     ]
    }
   ],
   "source": [
    "# create a user input for testing\n",
    "user_input = input(\"Enter a sentence: \")\n",
    "\n",
    "# Tokenize the user input\n",
    "x_test = tokenizer.texts_to_sequences([user_input])\n",
    "\n",
    "# Pad the user input\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Predict the class\n",
    "prediction = model.predict(x_test)\n",
    "\n",
    "# Print the prediction\n",
    "print(prediction)\n",
    "# Print out the prediction in human readable form\n",
    "print(\"The sentence is\", \"spam\" if np.argmax(prediction) == 1 else \"not spam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'to', 'and', 'of', 'a', 'in', \"'\", 'for', 'you', 'is', 'enron', 'this', 'on', 'that', 'i', 's', 'with', 'subject', 'be', 'your', 'we', 'as', 'it', 'from', 'have', 'will', 'are', 'ect', 'or', 'at', 'by', 'not', 'com', 'our', 'company', '1', 'if', 'all', 'an', '2', 'has', 'please', '3', 'can', 'hou', 'was', '2001', 'any', 'e', 'me', 'would', 'its', 'new', 'more', 'no', '10', '2000', 'am', 'my', '5', 't', 'but', 'information', 'may', 'said', 're', 'which', '00', 'do', 'about', 'they', 'business', 'energy', 'time', 'been', 'up', 'one', 'gas', '4', 'out', 'us', 'here', 'http', 'get', '0', '01', 'he', '000', 'these', 'their', 'message', 'pm', 'email', 'know', 'cc', 'there', '11', 'price', 'now', 'also']\n",
      "['ckh', 'ukh', '9890', 'milhalik', 'reapproach', 'dewatering', 'cloaking', 'guise', '412219', '454057', 'mousemillions', 'mousemaniacs', 'mydomain', '1434', '9277', '1221927', 'whitselk', 'perfoming', 'despatch', 'disconnects', 'illusions', 'formalize', 'mispelling', '60217', '286715', '687148', '683556', '683947', 'secom', '4280', '2001123110', 'fsa', 'turbeville', 'enthousiastic', '8422', 'internetweek', 'fittest', 'assimiliation', 'dayo', '35926', '2002010603', '61292292300', 'quilke', 'prebuild', '4785', '2002011707', 'sprit', 'worthty', 'ramu', 'chidambaram', 'ashok', 'hellmann', 'pancharatnam', 'ramaswamy', 'arvind', 'ritu', '200101286', 'rheadquarter', 'raddress', 'rcity', 'rstate', 'rzip', 'respecting', '7294', 'ochairm', 'piekielniak', 'wehring', 'gallishaw', 'lenderman', 'sarita', 'kudym', 'keiderling', 'alliece', 'alvis', 'gensichen', 'darla', 'svatos', '31819', '528403', '7461', 'dupuy', 'postlethwaite', 'marryott', 'madhup', 'skellett', 'umanoff', 'strives', 'emeeting', 'qualcom', '639921', '640346', 'bloombergs', '20010608440010', '20010608440012', '20010608440011', '20010608440009', '1659', '20010608440021', '6956', 'securityrequests']\n",
      "86701\n",
      "86701\n",
      "the 87821\n",
      "to 63365\n",
      "and 47107\n",
      "of 44374\n",
      "a 35634\n",
      "in 31951\n",
      "' 25931\n",
      "for 25245\n",
      "you 23710\n",
      "is 21308\n",
      "ungruff 5\n",
      "tannogen 5\n",
      "hymnarium 5\n",
      "zoonomia 5\n",
      "semiautomatics 5\n",
      "infinite 5\n",
      "rope 5\n",
      "expiore 5\n",
      "simpiicity 5\n",
      "qualquer 5\n",
      "[[8, 10, 5, 1677, 8298, 5714, 1028, 14, 19, 6, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Get the word counts for each word in the corpus\n",
    "word_counts = tokenizer.word_counts\n",
    "\n",
    "# Sort the word counts in descending order\n",
    "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the top most common words\n",
    "top_words = [word for word, count in sorted_word_counts[:100]]\n",
    "least_words = [word for word, count in sorted_word_counts[-100:]]\n",
    "\n",
    "# Print the 10 most common words\n",
    "print(top_words)\n",
    "print(least_words)\n",
    "\n",
    "# Print the length of the vocabulary\n",
    "print(len(tokenizer.word_counts))\n",
    "print(len(tokenizer.word_index))\n",
    "\n",
    "# Print the frequency of the top 10 most common words\n",
    "for word, count in sorted_word_counts[:10]:\n",
    "    print(word, count)\n",
    "    \n",
    "# Print the frequency of the top 10 least common words\n",
    "for word, count in sorted_word_counts[20000:20010]:\n",
    "\tprint(word, count)\n",
    " \n",
    "text = \"for simpiicity is a test sentence containing words that simpiicity  be in the tokenizer's vocabulary.\"\n",
    "tokenized_text = tokenizer.texts_to_sequences([text])\n",
    "print(tokenized_text)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
